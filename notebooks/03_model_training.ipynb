{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06bcc11a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Add src to path\u001b[39;00m\n\u001b[32m     17\u001b[39m sys.path.append(\u001b[38;5;28mstr\u001b[39m(Path().parent / \u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_preprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMSPreprocessor\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfeature_engineering\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FeatureEngineer\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel_training\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SpamClassifier\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'data_preprocessing'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model Training and Evaluation Notebook\n",
    "Train and evaluate the spam detection model\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score, learning_curve\n",
    "import joblib\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path().parent / \"src\"))\n",
    "\n",
    "from data_preprocessing import SMSPreprocessor\n",
    "from feature_engineering import FeatureEngineer\n",
    "from model_training import SpamClassifier\n",
    "import config\n",
    "\n",
    "# Load and preprocess data\n",
    "preprocessor = SMSPreprocessor()\n",
    "df = preprocessor.load_and_preprocess_data(config.SPAM_DATA_FILE)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = preprocessor.split_data(df)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "print(f\"Training spam ratio: {y_train.mean():.3f}\")\n",
    "print(f\"Test spam ratio: {y_test.mean():.3f}\")\n",
    "\n",
    "# Feature engineering\n",
    "feature_engineer = FeatureEngineer()\n",
    "X_train_vec = feature_engineer.fit_transform(X_train)\n",
    "X_test_vec = feature_engineer.transform(X_test)\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X_train_vec.shape}\")\n",
    "print(f\"Number of features: {X_train_vec.shape[1]}\")\n",
    "\n",
    "# Train model\n",
    "classifier = SpamClassifier()\n",
    "classifier.train(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "results = classifier.evaluate(X_test_vec, y_test)\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, results['predictions']))\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "cm = results['confusion_matrix']\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Confusion Matrix')\n",
    "axes[0, 0].set_xlabel('Predicted')\n",
    "axes[0, 0].set_ylabel('Actual')\n",
    "\n",
    "# 2. ROC Curve\n",
    "y_proba = classifier.predict_proba(X_test_vec)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "axes[0, 1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "axes[0, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "axes[0, 1].set_xlim([0.0, 1.0])\n",
    "axes[0, 1].set_ylim([0.0, 1.05])\n",
    "axes[0, 1].set_xlabel('False Positive Rate')\n",
    "axes[0, 1].set_ylabel('True Positive Rate')\n",
    "axes[0, 1].set_title('ROC Curve')\n",
    "axes[0, 1].legend(loc=\"lower right\")\n",
    "\n",
    "# 3. Feature Importance (Top 20 features)\n",
    "feature_names = feature_engineer.get_feature_names()\n",
    "feature_importance = np.abs(classifier.model.feature_log_prob_[1] - classifier.model.feature_log_prob_[0])\n",
    "top_indices = np.argsort(feature_importance)[-20:]\n",
    "\n",
    "axes[1, 0].barh(range(20), feature_importance[top_indices])\n",
    "axes[1, 0].set_yticks(range(20))\n",
    "axes[1, 0].set_yticklabels([feature_names[i] for i in top_indices])\n",
    "axes[1, 0].set_xlabel('Feature Importance')\n",
    "axes[1, 0].set_title('Top 20 Most Important Features')\n",
    "\n",
    "# 4. Cross-validation scores\n",
    "cv_scores = cross_val_score(classifier.model, X_train_vec, y_train, cv=5)\n",
    "axes[1, 1].bar(range(1, 6), cv_scores)\n",
    "axes[1, 1].set_xlabel('Fold')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].set_title(f'Cross-Validation Scores (Mean: {cv_scores.mean():.3f})')\n",
    "axes[1, 1].set_ylim([0.9, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Learning curve\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    classifier.model, X_train_vec, y_train, \n",
    "    cv=5, train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training score')\n",
    "plt.plot(train_sizes, np.mean(val_scores, axis=1), 'o-', label='Validation score')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Error analysis\n",
    "print(\"\\nError Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get misclassified examples\n",
    "y_pred = results['predictions']\n",
    "misclassified = X_test[y_test != y_pred]\n",
    "misclassified_labels = y_test[y_test != y_pred]\n",
    "misclassified_predictions = y_pred[y_test != y_pred]\n",
    "\n",
    "print(f\"Total misclassified: {len(misclassified)}\")\n",
    "print(f\"False positives (Ham predicted as Spam): {sum((misclassified_labels == 0) & (misclassified_predictions == 1))}\")\n",
    "print(f\"False negatives (Spam predicted as Ham): {sum((misclassified_labels == 1) & (misclassified_predictions == 0))}\")\n",
    "\n",
    "# Show some misclassified examples\n",
    "print(\"\\nSome misclassified examples:\")\n",
    "for i, (text, true_label, pred_label) in enumerate(zip(misclassified.head(5), \n",
    "                                                       misclassified_labels.head(5), \n",
    "                                                       misclassified_predictions.head(5))):\n",
    "    label_map = {0: 'Ham', 1: 'Spam'}\n",
    "    print(f\"\\n{i+1}. Text: {text}\")\n",
    "    print(f\"   True: {label_map[true_label]}, Predicted: {label_map[pred_label]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sms-spam-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
